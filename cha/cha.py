import re
import time
import requests
import yaml
import base64
from pagermaid.utils import pip_install

try:
    from bs4 import BeautifulSoup
except Exception as e:
    print(f"æ•è·åˆ°å¼‚å¸¸: {e}")
    
    
def install_if_missing(package_name):
    try:
        __import__(package_name)
        print(f"{package_name} å·²ç»å®‰è£…ã€‚")
    except ImportError:
        print(f"æœªæ‰¾åˆ° {package_name}ï¼Œæ­£åœ¨å®‰è£…...")
        subprocess.run(["pip3", "install", package_name], check=True)
        print(f"{package_name} å·²å®‰è£…ã€‚")

install_if_missing("bs4")

from bs4 import BeautifulSoup
from urllib import parse
from pagermaid.enums import Message
from urllib.parse import unquote
from pagermaid.listener import listener
from pagermaid.utils import alias_command
from pagermaid.dependence import client as http_client

# æ­¤ç‰ˆæœ¬æ˜¯ä¿®æ”¹ç‰ˆçš„ä¿®æ”¹ç‰ˆï¼Œ @fffffx2 ä¿®æ”¹å, @xream ä¿®æ”¹
# ä¿®å¤äº†åœ¨évenvç¯å¢ƒä¸‹ä½¿ç”¨pip installæŠ¥é”™çš„æƒ…å†µ @MintAby
def get_filename_from_url(url):
    if "sub?target=" in url:
        pattern = r"url=([^&]*)"
        match = re.search(pattern, url)
        if match:
            encoded_url = match.group(1)
            decoded_url = unquote(encoded_url)
            return get_filename_from_url(decoded_url)
    elif "api/v1/client/subscribe?token" in url:
        if "&flag=clash" not in url:
            url = url + "&flag=clash"
        else:
            pass
        try:
            response = requests.get(url)
            header = response.headers.get('Content-Disposition')
            if header:
                pattern = r"filename\*=UTF-8''(.+)"
                result = re.search(pattern, header)
                if result:
                    filename = result.group(1)
                    filename = parse.unquote(filename)  # å¯¹æ–‡ä»¶åè¿›è¡Œè§£ç 
                    airport_name = filename.replace("%20", " ").replace("%2B", "+")
                    return airport_name
        except:
            return 'æœªçŸ¥'
    else:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (HTML, like Gecko) '
                          'Chrome/108.0.0.0'
                          'Safari/537.36'}
        try:
            pattern = r'(https?://)([^/]+)'
            match = re.search(pattern, url)
            base_url = None
            if match:
                base_url = match.group(1) + match.group(2)
            response = requests.get(url=base_url + '/auth/login', headers=headers, timeout=10)
            if response.status_code != 200:
                response = requests.get(base_url, headers=headers, timeout=1)
            html = response.content
            soup = BeautifulSoup(html, 'html.parser')
            title = soup.title.string
            title = str(title).replace('ç™»å½• â€” ', '')
            if "Attention Required! | Cloudflare" in title:
                title = 'è¯¥åŸŸåä»…é™å›½å†…IPè®¿é—®'
            elif "Access denied" in title or "404 Not Found" in title:
                title = 'è¯¥åŸŸåéæœºåœºé¢æ¿åŸŸå'
            elif "Just a moment" in title:
                title = 'è¯¥åŸŸåå¼€å¯äº†5sç›¾'
            else:
                pass
            return title
        except:
            return 'æœªçŸ¥'


def convert_time_to_str(ts):
    return str(ts).zfill(2)


def sec_to_data(y):
    h = int(y // 3600 % 24)
    d = int(y // 86400)
    h = convert_time_to_str(h)
    d = convert_time_to_str(d)
    return d + "å¤©" + h + "å°æ—¶"


def StrOfSize(size):
    def strofsize(integer, remainder, level):
        if integer >= 1024:
            remainder = integer % 1024
            integer //= 1024
            level += 1
            return strofsize(integer, remainder, level)
        elif integer < 0:
            integer = 0
            return strofsize(integer, remainder, level)
        else:
            return integer, remainder, level

    units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']
    integer, remainder, level = strofsize(size, 0, 0)
    if level + 1 > len(units):
        level = -1
    return ('{}.{:>03d} {}'.format(integer, remainder, units[level]))


def get_node_count(url, headers):
    try:
        res = requests.get(url, headers=headers, timeout=5)
        if res.status_code == 200:
            try:
                # å°è¯•è§£æä¸ºYAMLæ ¼å¼
                config = yaml.safe_load(res.text)
                if config and 'proxies' in config:
                    return len(config['proxies'])
            except:
                # å¦‚æœä¸æ˜¯YAMLæ ¼å¼ï¼Œå°è¯•è®¡ç®—èŠ‚ç‚¹æ•°é‡
                node_patterns = [
                    'vmess://', 'trojan://', 'ss://', 'ssr://',
                    'vless://', 'hy2://', 'hysteria://', 'hy://'
                ]
                node_count = sum(len(re.findall(pattern, res.text)) for pattern in node_patterns)
                return node_count if node_count > 0 else 'æœªçŸ¥'
    except:
        return 'æœªçŸ¥'
    return 'æœªçŸ¥'


def calculate_percentage(used, total):
    if total == 0:
        return 0
    return round((used / total) * 100, 2)


def parse_subscription_info(url, headers):
    try:
        res = requests.get(url, headers=headers, timeout=5)
        if res.status_code == 200:
            # é¦–å…ˆå°è¯•ä½œä¸ºClashé…ç½®è§£æ
            try:
                config = yaml.safe_load(res.text)
                if config and 'proxies' in config:
                    # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹
                    type_count = {}
                    regions = {}
                    
                    for proxy in config['proxies']:
                        # ç»Ÿè®¡ç±»å‹
                        proxy_type = proxy.get('type', '').lower()
                        type_count[proxy_type] = type_count.get(proxy_type, 0) + 1
                        
                        # è§£æåœ°åŒºä¿¡æ¯
                        name = proxy.get('name', '').lower()
                        # åœ°åŒºåŒ¹é…è§„åˆ™
                        region_rules = [
                            ('é¦™æ¸¯', ['é¦™æ¸¯', 'hong kong', 'hongkong', 'hk', 'ğŸ‡­ğŸ‡°']),
                            ('å°æ¹¾', ['å°æ¹¾', 'taiwan', 'tw', 'ğŸ‡¹ğŸ‡¼']),
                            ('æ—¥æœ¬', ['æ—¥æœ¬', 'japan', 'jp', 'ğŸ‡¯ğŸ‡µ']),
                            ('æ–°åŠ å¡', ['æ–°åŠ å¡', 'singapore', 'sg', 'ğŸ‡¸ğŸ‡¬']),
                            ('ç¾å›½', ['ç¾å›½', 'united states', 'us', 'usa', 'ğŸ‡ºğŸ‡¸']),
                            ('éŸ©å›½', ['éŸ©å›½', 'korea', 'kr', 'ğŸ‡°ğŸ‡·']),
                            ('å¾·å›½', ['å¾·å›½', 'germany', 'de', 'ğŸ‡©ğŸ‡ª']),
                            ('è‹±å›½', ['è‹±å›½', 'united kingdom', 'uk', 'ğŸ‡¬ğŸ‡§'])
                        ]
                        
                        for region_name, keywords in region_rules:
                            if any(keyword in name for keyword in keywords):
                                regions[region_name] = regions.get(region_name, 0) + 1
                                break
                    
                    return {
                        'type_count': type_count,
                        'regions': regions
                    }
            except yaml.YAMLError:
                pass
            
            # å¦‚æœä¸æ˜¯Clashé…ç½®ï¼Œå°è¯•base64è§£ç 
            try:
                decoded_content = base64.b64decode(res.text).decode('utf-8')
            except:
                return None
            
            # ç»Ÿè®¡èŠ‚ç‚¹ç±»å‹
            type_count = {
                'vmess': 0,
                'ss': 0,
                'ssr': 0,
                'trojan': 0,
                'vless': 0,
                'hy2': 0,
                'hysteria': 0,
                'hy': 0
            }
            
            # è§£æåœ°åŒºä¿¡æ¯
            regions = {}
            
            # åˆ†ææ¯ä¸€è¡Œ
            lines = decoded_content.split('\n')
            for line in lines:
                if not line.strip():
                    continue
                
                # æ£€æµ‹èŠ‚ç‚¹ç±»å‹
                if line.startswith('vmess://'):
                    type_count['vmess'] += 1
                    try:
                        vmess_info = base64.b64decode(line[8:]).decode('utf-8')
                        line = vmess_info
                    except:
                        pass
                elif line.startswith('ss://'):
                    type_count['ss'] += 1
                elif line.startswith('ssr://'):
                    type_count['ssr'] += 1
                    try:
                        ssr_info = base64.b64decode(line[6:]).decode('utf-8')
                        line = ssr_info
                    except:
                        pass
                elif line.startswith('trojan://'):
                    type_count['trojan'] += 1
                elif line.startswith('vless://'):
                    type_count['vless'] += 1
                elif line.startswith('hy2://'):
                    type_count['hy2'] += 1
                elif line.startswith('hysteria://'):
                    type_count['hysteria'] += 1
                elif line.startswith('hy://'):
                    type_count['hy'] += 1
                
                # è§£æåœ°åŒºä¿¡æ¯
                line_lower = line.lower()
                region_rules = [
                    ('é¦™æ¸¯', ['é¦™æ¸¯', 'hong kong', 'hongkong', 'hk', 'ğŸ‡­ğŸ‡°']),
                    ('å°æ¹¾', ['å°æ¹¾', 'taiwan', 'tw', 'ğŸ‡¹ğŸ‡¼']),
                    ('æ—¥æœ¬', ['æ—¥æœ¬', 'japan', 'jp', 'ğŸ‡¯ğŸ‡µ']),
                    ('æ–°åŠ å¡', ['æ–°åŠ å¡', 'singapore', 'sg', 'ğŸ‡¸ğŸ‡¬']),
                    ('ç¾å›½', ['ç¾å›½', 'united states', 'us', 'usa', 'ğŸ‡ºğŸ‡¸']),
                    ('éŸ©å›½', ['éŸ©å›½', 'korea', 'kr', 'ğŸ‡°ğŸ‡·']),
                    ('å¾·å›½', ['å¾·å›½', 'germany', 'de', 'ğŸ‡©ğŸ‡ª']),
                    ('è‹±å›½', ['è‹±å›½', 'united kingdom', 'uk', 'ğŸ‡¬ğŸ‡§'])
                ]
                
                for region_name, keywords in region_rules:
                    if any(keyword in line_lower for keyword in keywords):
                        regions[region_name] = regions.get(region_name, 0) + 1
                        break
            
            # è¿‡æ»¤æ‰æ•°é‡ä¸º0çš„ç±»å‹å’Œåœ°åŒº
            type_count = {k: v for k, v in type_count.items() if v > 0}
            regions = {k: v for k, v in regions.items() if v > 0}
            
            return {
                'type_count': type_count,
                'regions': regions
            }
    except Exception as e:
        print(f"è§£æé”™è¯¯: {str(e)}")
        return None


@listener(is_plugin=True, outgoing=True, command=alias_command("cha"),
          description='è¯†åˆ«è®¢é˜…é“¾æ¥å¹¶è·å–ä¿¡æ¯\nä½¿ç”¨æ–¹æ³•ï¼šä½¿ç”¨è¯¥å‘½ä»¤å‘é€æˆ–å›å¤ä¸€æ®µå¸¦æœ‰ä¸€æ¡æˆ–å¤šæ¡è®¢é˜…é“¾æ¥çš„æ–‡æœ¬',
          parameters='<url>')
async def subinfo(_, msg: Message):
    headers = {
        'User-Agent': 'ClashforWindows/0.18.1'
    }
    output_text = None
    try:
        message_raw = msg.reply_to_message and (msg.reply_to_message.caption or msg.reply_to_message.text) or (
                    msg.caption or msg.text)
        final_output = ''
        url_list = re.findall("https?://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]",
                              message_raw)
        for url in url_list:
            try:
                res = await http_client.get(url, headers=headers, timeout=5)
                while res.status_code == 301 or res.status_code == 302:
                    url1 = res.headers['location']
                    res = await http_client.get(url1, headers=headers, timeout=5)
            except:
                final_output = final_output + 'è¿æ¥é”™è¯¯' + '\n\n'
                continue
            if res.status_code == 200:
                try:
                    info = res.headers['subscription-userinfo']
                    info_num = re.findall('\d+', info)
                    time_now = int(time.time())
                    
                    # è®¡ç®—ä½¿ç”¨ç™¾åˆ†æ¯”
                    used_traffic = int(info_num[0]) + int(info_num[1])
                    total_traffic = int(info_num[2])
                    usage_percent = calculate_percentage(used_traffic, total_traffic)
                    
                    # è·å–èŠ‚ç‚¹æ•°é‡
                    node_count = get_node_count(url, headers)
                    
                    # è·å–è®¢é˜…æ›´æ–°æ—¶é—´
                    last_modified = res.headers.get('last-modified', 'æœªçŸ¥')
                    
                    # è·å–èŠ‚ç‚¹ç±»å‹å’Œåœ°åŒºä¿¡æ¯
                    sub_info = parse_subscription_info(url, headers)
                    type_info = ''
                    region_info = ''
                    
                    if sub_info:
                        # ç”ŸæˆèŠ‚ç‚¹ç±»å‹ä¿¡æ¯
                        type_counts = sub_info['type_count']
                        type_strings = []
                        for node_type, count in type_counts.items():
                            if count > 0:
                                type_strings.append(f'{node_type}:{count}')
                        type_info = 'èŠ‚ç‚¹ç±»å‹ï¼š`' + ', '.join(type_strings) + '`\n'
                        
                        # ç”Ÿæˆåœ°åŒºä¿¡æ¯
                        region_counts = sub_info['regions']
                        region_strings = []
                        for region, count in region_counts.items():
                            region_strings.append(f'{region}:{count}')
                        region_info = 'èŠ‚ç‚¹åœ°åŒºï¼š`' + ', '.join(region_strings) + '`\n'
                    
                    output_text_head = (
                        f'è®¢é˜…é“¾æ¥ï¼š`{url}`\n'
                        f'æœºåœºåï¼š`{get_filename_from_url(url)}`\n'
                        f'å·²ç”¨ä¸Šè¡Œï¼š`{StrOfSize(int(info_num[0]))}`\n'
                        f'å·²ç”¨ä¸‹è¡Œï¼š`{StrOfSize(int(info_num[1]))}`\n'
                        f'å‰©ä½™ï¼š`{StrOfSize(int(info_num[2]) - int(info_num[1]) - int(info_num[0]))}`\n'
                        f'æ€»å…±ï¼š`{StrOfSize(int(info_num[2]))}`\n'
                        f'ä½¿ç”¨æ¯”ä¾‹ï¼š`{usage_percent}%`\n'
                        f'èŠ‚ç‚¹æ•°é‡ï¼š`{node_count}`\n'
                        f'{type_info}'
                        f'{region_info}'
                    )
                    
                    if len(info_num) >= 4:
                        timeArray = time.localtime(int(info_num[3]) + 28800)
                        dateTime = time.strftime("%Y-%m-%d", timeArray)
                        if time_now <= int(info_num[3]):
                            lasttime = int(info_num[3]) - time_now
                            output_text = output_text_head + '\næ­¤è®¢é˜…å°†äº`' + dateTime + '`è¿‡æœŸ' + 'ï¼Œå‰©ä½™`' + sec_to_data(
                                lasttime) + '`'
                        elif time_now > int(info_num[3]):
                            output_text = output_text_head + '\næ­¤è®¢é˜…å·²äº`' + dateTime + '`è¿‡æœŸï¼'
                    else:
                        output_text = output_text_head + '\nåˆ°æœŸæ—¶é—´ï¼š`æœªçŸ¥`'
                except:
                    output_text = 'è®¢é˜…é“¾æ¥ï¼š`' + url + '`\næœºåœºåï¼š`' + get_filename_from_url(url) + '`\næ— æµé‡ä¿¡æ¯'
            else:
                output_text = 'æ— æ³•è®¿é—®'
            final_output = final_output + output_text + '\n\n'
        await msg.edit(final_output)
    except:
        await msg.edit('å‚æ•°é”™è¯¯')
